{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6_9_machine_learning_practice2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNZ1dreQ44UZqR4PaTidsQd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ansehoon1999/hands-on_practice/blob/main/6_9_machine_learning_practice2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbLuXlsnNP_w"
      },
      "source": [
        "#6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm908xfxNYEY"
      },
      "source": [
        "1. 결정트리\n",
        "- 분류와 회귀를 작업할 수 있는 다재다능한 알고리즘임\n",
        "- 스무고개처럼 계속해서 질문해 나가면서 학습하는 방식\n",
        "\n",
        "2. 불순도\n",
        "- 한 노드의 샘플이 모두 같은 샘플에 포함되어있다면 순수하다고 하지만 그렇지 않으면 불순하다고 함\n",
        "\n",
        "3. CART 훈련 알고리즘\n",
        "- 하나의 특성 K 임곗값을 바탕으로 두 개의 서브셋으로 나누는 탐욕적 알고리즘\n",
        "\n",
        "4. 엔트로피\n",
        "- 얼마나 무질서한 정도\n",
        "\n",
        "5. 규제 매개변수\n",
        "- 비파라미터 모델: 훈련되기 전에 파라미터 수가 정해지지 않음\n",
        "- 파라미터 모델: 미리 정의된 모델 파라미터 수를 가지고 자유도가 제한 된다. \n",
        "\n",
        "6. 백만 개의 데이터로 학습시킬 떄 깊이는 어느정도?\n",
        "- M개의 리프노드를 포함한 이진 트리의 깊이는 로그 M을 반올림한 것과 같다.\n",
        "\n",
        "\n",
        "7. 한 노드의 지니 불순도 부모 노드 VS 자식노드\n",
        "- 자식노드가 더 낮다. 자식 노드는 불순도를 최소화하는 방향으로 각 노드를 분할하고 cart 훈련 알고리즘 비용함수를 최소화하기 때문입니다.\n",
        "\n",
        "8. max_depth를 낮춰서 과대 적합을 방지할 수 있나?\n",
        "- 규제를 통해서 max_depth를 낮출 수 있다\n",
        "\n",
        "9. 과소적합이 되었다고 스케일을 조정해야 하는가?\n",
        "- 결정트리를 스케일하는 것은 과소적합과 관련이 없다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dUMCgkkNYBA"
      },
      "source": [
        "#7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWo71mZ_NX9f"
      },
      "source": [
        "1. 투표 기반의 분류기\n",
        "- 각 분류기의 예측을 통해 가장 많이 선택된 클래스를 예측하는 것\n",
        "- 약한 학습기가 여러 개 모인다면 강한 학습기를 만들 수 있음\n",
        "\n",
        "2. 큰 수의 법칙\n",
        "- 주사위 한 개를 n번 던지고 1의 눈이 x번 나왔다고 가정을 했을 때 n번이 무수히 많아진다면 수학적 확률에 가까워 진다.\n",
        "\n",
        "\n",
        "3. 직접 투표 VS 간접 투표\n",
        "- 직접 투표: 예측한 분류기들이 선택한 클래스 중에 가장 많이 선택한 클래스를 예측하는 것\n",
        "- 간접 투표: 모든 분류기가 클래스 확률을 예측하고 평균내어 가장 높은 클래스를 선택하는 것\n",
        "\n",
        "\n",
        "4. 배깅 VS 페이스팅\n",
        "- 배깅: 훈련세트의 중복을 허용하는 샘플링 방식\n",
        "- 페이스팅: 훈련세트의 중복을 허용하지 않는 샘플링 방식\n",
        "부트스트래핑은 각 예측기의 서브셋 다양성을 증가시키므로 배깅이 페이스팅보다 분산이 낮지만 편향은 조금 높다.\n",
        "\n",
        "\n",
        "5. OOB 평가\n",
        "배깅을 사용하게 되면 한 샘플이 여러 번 샘플링 된다.\n",
        "\n",
        "6. 랜덤 패치 방식 VS 랜덤 서브스페이스\n",
        "- 랜덤 패치 방식: 특성과 샘플을 모두 샘플링하는 방식\n",
        "- 랜덤 스페이스: 훈련샘플은 모두 사용하고 특성은 샘플링하는 방식\n",
        "\n",
        "\n",
        "7. 랜덤 포레스트\n",
        "- 배깅 방법을 적용한 결정 트리 앙상블\n",
        "\n",
        "8. 익스트림 랜덤 트리\n",
        "- 트리를 더욱 무작위하게 하는 대신에 후보 특성을 사용해 무작위로 분할한 다음에 그 중 최상의 분할을 선택\n",
        "\n",
        "- 편향은 크지만 분산은 낮아진다.\n",
        "\n",
        "\n",
        "9. 부스팅\n",
        "- 약한 학습기를 여러 개 돌리면서 강한 학습기를 만드는 방법\n",
        "- 에이다 부스트: 이전 예측기를 보안하는 새로운 방법: 과소적합 되었던 훈련 샘플의 가중치를 높이기\n",
        "- 그레이디언트 부스팅: 앙상블 이전까지의 오차를 보정하도록 예측을 순차적으로 추가\n",
        "\n",
        "10. 스태킹\n",
        "\n",
        "- 취합하는 모델을 훈련시킬 수 없을까?\n",
        "\n",
        "11, 배깅 앙상블 훈련을 여러 서버에 분산할 수 있을까?\n",
        "\n",
        "12. 엑스트라 트리 > 랜덤 포레스트\n",
        "- 각 노드의 특성의 일부를 무작위로 선택해 분할\n",
        "- 랜덤한 임곗값을 사용함\n",
        "\n",
        "13. 에이다 부스팅 앙상블 훈련이 과소적합?\n",
        "- 예측기의 수를 증가시키거나 규제 파라미터를 감소, 학습률을 증가\n",
        "\n",
        "14. 과소 적합?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faqnpEljNX6N"
      },
      "source": [
        "#8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Riyic4YNNX3e"
      },
      "source": [
        "1. 투영과 매니폴드 학습\n",
        "- 투영: 고차원 특성을 저차원의 부분 공간으로 위치시키는 것을 말함.\n",
        "- 매니폴드 학습: 대역적으로 독특한 수학적 위상을 가지는 매니폴드를 모델링하는 방식으로 고차원인 실제 데이터셋을 저차원 매니폴드로 놓이는 방법 \n",
        "\n",
        "2. PCA\n",
        "- 훈련 세트에서 분산이 최대인 축을 찾는 과정\n",
        "- I번째의 축을 데이터의 I번 째 주성분이라고 함.\n",
        "\n",
        "3. 설명된 분산의 비율\n",
        "- 각 주성분의 축을 따라 이어버리는 데이터셋의 분산 비율\n",
        "\n",
        "4. 재구성 오차\n",
        "- 원본 데이터와 재구성된 데이터 사이의 평균 제곱 거리\n",
        "\n",
        "5. 랜덤 PCA\n",
        "- 확률적 알고리즘을 사용해 처음 d개의 주성분에 대한 근삿값을 빠르게 찾아냄\n",
        "\n",
        "6. 점진적 PCA\n",
        "- 미니 배치 방식으로 알고리즘에 한 번에 하나씩 주입\n",
        "\n",
        "7. 커널 PCA\n",
        "-  커널 트릭: 샘플을 매우 높은 공간에 매핑하여 서포트 벡터 머신의 비선형 회귀를 가능하게 함.\n",
        "\n",
        "8. LLE (지역 선형 임베딩)\n",
        "- 투영에 의존하지 않는 매니폴드 학습 방식으로 가까운 이웃에 얼마나 선형적으로 연관되어 있는지를 측정\n",
        "\n",
        "9. 차원 축소의 목적\n",
        "- 속도를 빠르게 할 수 있음\n",
        "- 시각화 가능\n",
        "- 메모리 공간의 절약\n",
        "\n",
        "10. 단점\n",
        "- 성능 감소의 가능성이 있음\n",
        "- 계산 비용이 증가\n",
        "- 파이프라인이 복잡해짐\n",
        "\n",
        "\n",
        "11. 차원의 저주\n",
        "- 저차원에서 없던 문제가 고차원에서 발생되는 문제\n",
        "- 고차원 벡터는 매우 희소해서 과대적합의 위험이 크고, 많은 데이터가 없다면 패턴을 찾아내기 힘들다.\n",
        "\n",
        "12. 데이터 복원\n",
        "- 완벽한 데이터 복원은 불가능하다.\n",
        "\n",
        "13. 매우 비선형적인 데이터셋의 차원을 축소하는데 PCA를 사용하는가?\n",
        "\n",
        "14. 언제 어떤 PCA를 사용하나요?\n",
        "- 기본 PCA: 데이터 크기가 메모리에 맞을 때\n",
        "- 점진적 PCA: 메모리에 담을 수 없는 대용량 데이터셋\n",
        "- 랜덤 PCA: 데이터 셋이 메모리 크기에 맞고 차원을 크게 감소시킬 때\n",
        "- 커널 PCA\n",
        "\n",
        "15. 차원 축소 알고리즘 성능 평가\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IXWutsjNXjx"
      },
      "source": [
        "#9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtdYG1tgT2A_"
      },
      "source": [
        "1. 군집\n",
        "- 비슷한 샘플을 그룹(클러스터)로 할당하는 것\n",
        "\n",
        "2. K-평균\n",
        "- 센트로이드를 랜덤하게 설정하고 레이블을 할당하고 센트로이드를 업데이트 하면서 변화가 없을 때까지 계속하는 과정\n",
        "\n",
        "3. 이너셔\n",
        "- 샘플과 가장 가까운 센트로이드 사이의 평균 제곱 거리, 엘보를 통해서 K값을 찾는다.\n",
        "\n",
        "4. 실루엣 점수: b-a / (max(a, b)) \n",
        "- a: 동일한 클러슽에서 다른 샘플까지의 평균 거리\n",
        "- b: 가장 가까운 클러스터의 평균 거리\n",
        "- +1에 가까울 수록 잘 속해있음.\n",
        "\n",
        "\n",
        "5. 군집을 사용한 준지도 학습\n",
        "- 레이블이 없는 데이터가 많고 레이블이 있는 데이터는 적기 때문에 동일한 클러스터에 있는 모든 샘플로 전파(레이블 전파)\n",
        "\n",
        "6. DBSCAN\n",
        "- 밀집된 지역을 바탕으로 군집화 하는 것\n",
        "\n",
        "7. 가우시안 혼합 모델\n",
        "- 파라미터가 알려지지 않는 여러 개의 혼합된 가우시안 분포에서 생성되었다고 가정하는 모델\n",
        "- 가우시안에서 생성된 모든 샘플은 하나의 클러스터를 형성\n",
        "- BIC/AIC 그래프를 최소화하는 부분의 클러스터 개수를 찾거나 베이즈 가우시안 혼합 모델을 사용해 클러스터의 개수를 자동 선택하는 것\n",
        "\n",
        "\n"
      ]
    }
  ]
}