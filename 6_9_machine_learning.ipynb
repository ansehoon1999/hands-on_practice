{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6_9_machine_learning.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNoC8d7neIRnuzlb7qlweeJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ansehoon1999/hands-on_practice/blob/main/6_9_machine_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbLuXlsnNP_w"
      },
      "source": [
        "#6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm908xfxNYEY"
      },
      "source": [
        "1. 결정트리\n",
        "- 분류와 회귀를 작업할 수 있는 다재다능한 알고리즘\n",
        "\n",
        "2. 불순도\n",
        "- 한 노드의 샘플이 모두 같은 클래스에 속해 있다면 이 노드를 순수하다고 한다\n",
        "\n",
        "3. CART 훈련 알고리즘\n",
        "- 훈련 세트를 하나의 특성 K의 임곗값을 사용해 두개의 서브셋으로 나누는 알고리즘, 탐욕적 알고리즘 이다.\n",
        "\n",
        "4. 엔트로피\n",
        "- 분자의 무질서한 정도를 말한다.\n",
        "\n",
        "5. 규제 매개변수\n",
        "- 비파라미터 모델: 훈련되기전에 파라미터의 수가 정해지지 않음\n",
        "- 파라미터 모델: 미리 정의된 모델 파라미터 수를 가지고 자유도가 제한되지만 과대적합이 될 위험도가 낮다.\n",
        "\n",
        "6. 백만 개의 데이터로 학습시킬 떄 깊이는 어느정도?\n",
        "- M개의 리프노트들 포함한 이진 트리의 깊이는 로그 M을 반올림 한 것과 같다\n",
        "\n",
        "7. 한 노드의 지니 불순도 부모 노드 VS 자식노드\n",
        "- 일반적으로 부모 노드의 불순도 보다 낮음, 이는 자식의 불순도 가중치 합이 최소화되는 방향으로 각 노드를 분할하는 cart 훈련 알고리즘 비용함 수 때문이다.\n",
        "\n",
        "8. max_depth를 낮춰서 과대 적합을 방지할 수 있나?\n",
        "- 규제를 통해서 max_depth를 낮출 수 있다.\n",
        "\n",
        "9. 과소적합이 되었다고 스케일을 조정해야 하는가?\n",
        "- 결정트리를 스케일하는 것은 과소적합과 관련이 없다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dUMCgkkNYBA"
      },
      "source": [
        "#7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWo71mZ_NX9f"
      },
      "source": [
        "1. 투표 기반의 분류기\n",
        "- 각 분류기의 예측을 모아서 가장 많이 선택된 클래스를 예측하는 것\n",
        "- 약한 학습기 여러개로도 강한 학습기를 만들 수 있음\n",
        "\n",
        "2. 큰 수의 법칙\n",
        "- 주사위 한 개를 n번 던지고 1의 눈이 x번 나왔다면\n",
        "P(|X/N - 1/6| < h) = 1\n",
        "- n이 한없이 커질수록 수학적 확률에 가까워 진다.\n",
        "\n",
        "3. 직접 투표 VS 간접 투표\n",
        "- 직접 투표: 예측한 분류기들이 선택한 클래스 중 가장 많이 선택한 클래스를 예측하는 것\n",
        "- 간접 투표: 모든 분류기가 클래스 확률을 예측할 수 있으며 예측을 평균내어 가장 높은 클래스를 선택\n",
        "\n",
        "4. 배깅 VS 페이스팅\n",
        "\n",
        "-  배깅: 훈련 세트에서 중복을 허용하여 샘플링 하는 방식\n",
        "-  페이스팅: 훈련 세트에서 중복을 허용하지 않는 샘플링 방식\n",
        "-  부트스트래핑은 각 예측기를 학습하는 서브셋의 다양성을 증가시키므로 배깅이 페이스팅보다 분산이 낮지만 편향은 조금 높음.\n",
        "\n",
        "5. OOB 평가\n",
        "- 배깅을 사용하게 되면 어떤 샘플은 한 예측기를 위해 여러 번 샘플링하고 어떤건 선택이 되지 않는다.\n",
        "\n",
        "6. 랜덤 패치 방식 VS 랜덤 서브스페이스\n",
        "- 특성과 샘플을 모두 샘플링하는 것\n",
        "- 훈련 샘플을 모두 사용하고 특성은 샘플링하는 것\n",
        "\n",
        "7. 랜덤 포레스트\n",
        "- 배깅 방법을 적용한 결정 트리 앙상블\n",
        "\n",
        "8. 익스트림 랜덤 트리\n",
        "- 트리를 더욱 무작위하게 하는 대신에 후보 특성을 사용해 무작위로 분할한 다음 그 중 최상의 분할을 선택\n",
        "\n",
        "- 편향이 늘어나지만 분산을 낮추게 한다\n",
        "\n",
        "9. 부스팅\n",
        "- 약한 학습기를 여러 개 돌리면서 강한 학습기를 만드는 방법\n",
        "- 에이다 부스트: 이전 예측기를 보안하는 새로운 방법: 과소적합되었던 훈련 샘플의 가중치를 높이기\n",
        "- 그레이디언트 부스팅: 앙상블 이전까지의 오차를 보정하도록 예측을 순차적으로 추가\n",
        "10. 스태킹\n",
        "- 취합하는 모델을 훈련시킬 수는 없을까 해서 나온 것\n",
        "\n",
        "\n",
        "11, 배깅 앙상블 훈련을 여러 서버에 분산할 수 있을까?\n",
        "- 각 예측기는 독립적이기 때문에 가능함\n",
        "\n",
        "12. 엑스트라 트리 > 랜덤 포레스트\n",
        "- 각 노드의 특성의 일부를 무작위로 선택해 분할, 최선의 임곗값을 찾아내는 것이 아니라 랜덤한 임곗값 사용\n",
        "\n",
        "13. 에이다 부스팅 앙상블 훈련이 과소적합?\n",
        "- 예측기의 수를 증가시키거나 규제 파라미터의를 감소시키고, 학습률을 증가\n",
        "\n",
        "14. 과소 적합?\n",
        "- 학습률을 감소시켜야함, 알맞은 개수를 찾기 위해 조기 종료 기법을 사용하는 것도 방법임\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faqnpEljNX6N"
      },
      "source": [
        "#8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Riyic4YNNX3e"
      },
      "source": [
        "1. 투영과 매니폴드 학습\n",
        "\n",
        "- 투영: 특성들은 서로 강하게 연결되어 있는 반면 대부분은 변화가 없음\n",
        "- 매니폴드 학습: 많은 차원 축소 알고리즘이 훈련 샘플이 놓여 있는 매니폴드를 모델링함, \n",
        "- 대부분 실제 고차원 데이터 셋이 더 낮은 저차원 매니폴드에 가깝게 놓여 있다는 매니폴드 가정에 근거함\n",
        "\n",
        "2. PCA\n",
        "- 훈련 세트에서 분산이 최대인 축을 찾는 과정\n",
        "- I번째의 축을 데이터의 I번째 주성분이라고 함\n",
        "\n",
        "3. 설명된 분산의 비율\n",
        "- 각 주성분의 축을 따라 있는 데이터셋의 분산 비율\n",
        "\n",
        "4. 재구성 오차\n",
        "- 원본 데이터와 재구성된 데이터 사이의 평균 제곱 거리\n",
        "\n",
        "5. 랜덤 PCA\n",
        "- 확률적 알고리즘을 사용해 처음 D개의 주성분에 대한 근삿값을 빠르게 찾아냄\n",
        "\n",
        "6. 점진적 PCA\n",
        "- 미니 배치 방식으로 알고리즘에 한 번에 하나씩 주입\n",
        "\n",
        "7. 커널 PCA\n",
        "-  커널 트릭: 샘플을 매우 높은 공간에 매핑하여 서포트 벡터 머신의 비선형 분류와 회귀를 가능하게 함\n",
        "- 커널 PCA: PCA를 적용해 차원 축소를 위한 복잡한 비선형 투형\n",
        "\n",
        "8. LLE (지역 선형 임베딩)\n",
        "- 투영에 의존하지 않는 매니폴드 학습, 각 훈련 샘플이 가장 가까운 이웃에 얼마나 선형적으로 연관되어 있는지 측정\n",
        "\n",
        "9. 차원 축소의 목적\n",
        "- 알고리즘 속도를 높이기 위해서\n",
        "- 메모리 공간의 절약\n",
        "- 데이터 시각화로 중요한 특성을 통찰\n",
        "\n",
        "10. 단점\n",
        "\n",
        "- 성능 감소의 가능성이 있음\n",
        "- 계산 비용 증가\n",
        "- 파이프라인의 복잡도가 증가함\n",
        "\n",
        "11. 차원의 저주\n",
        "\n",
        "- 저차원 공간에서 없던 문제가 고차원에서 발생하는 문제\n",
        "- 고차원 벡터는 매우 희소해서 과대적합의 위험이 크고, 많은 데이터가 있지 않으면 데이터에 있는 패턴을 찾아내기 힘들다.\n",
        "\n",
        "12. 데이터 복원\n",
        "- 차원이 축소되면 일부가 사라지기 때문에 완벽히 되돌리는 것은 불가능하다.\n",
        "\n",
        "13. 매우 비선형적인 데이터셋의 차원을 축소하는데 PCA를 사용하는가?\n",
        "- 불필요한 차원을 제거할 수 있기 때문에 차원을 축소한다.\n",
        "\n",
        "14. 언제 어떤 PCA를 사용하나요?\n",
        "- 기본 PCA: 데이터 크기가 메모리에 맞을 때\n",
        "- 점진적 PCA: 메모리에 담을 수 없는 대용량 데이터셋\n",
        "- 랜덤 PCA: 데이터셋이 메모리 크기에 맞고 차원을 크게 축소시킬 때\n",
        "- 커널 PCA: 비선형 데이터셋\n",
        "\n",
        "15. 차원 축소 알고리즘 성능 평가\n",
        "- 매우 많은 정보를 잃지 않고 차원을 제거할 수 있다면 잘 작동한 것임\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IXWutsjNXjx"
      },
      "source": [
        "#9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtdYG1tgT2A_"
      },
      "source": [
        "1. 군집\n",
        "- 비슷한 샘플을 그룹(클러스터)로 할당하는 것\n",
        "- 고객 분류, 추천 시스템, 차원 축소, 이상치 탐색에 사용됨\n",
        "\n",
        "2. K-평균\n",
        "- 센트로이드를 랜덤하게 설정하고 레이블을 할당하고 센트로이드를 업데이트 하면서 센트로이드의 변화가 없을 때까지 계속하는 과정\n",
        "\n",
        "3. 이너셔\n",
        "- 샘플과 가장 가까운 센트로이드 사이의 평균 제곱 거리 -> 엘보를 통해 클러스터 개수르르 찾음\n",
        "\n",
        "4. 실루엣 점수: b-a / (max(a, b)) \n",
        "- a: 동일한 클러스터에 있는 다른 샘플까지의 평균 거리\n",
        "- b: 가장 가까운 클러스터의 평균 거리\n",
        "- 1에 가까울 수록 잘 속해있음\n",
        "\n",
        "5. 군집을 사용한 준지도 학습\n",
        "- 레이블이 없는 데이터가 많고 레이블이 있는 데이터가 적을 때 사용함.\n",
        "- 동일한 클러스터에 있는 모든 샘플로 전파(레이블 전파)\n",
        "\n",
        "6. DBSCAN\n",
        "- 밀집된 연속 지역을 클러스터로 정의함\n",
        "\n",
        "7. 가우시안 혼합 모델\n",
        "- 파라미터가 알려지지 않는 여러개의 혼합된 가우시안 분포에서 생성되었다고 가정하는 모델\n",
        "- 가우시안에서 생성된 모든 샘플은 하나의 클러스터를 형성한다(타원)\n",
        "\n",
        "- 가우시안 혼합 모델에서는 BIC/AIC 그래프를 기려 최소화되는 부분의 클러스터 개수를 찾거나 베이즈 가우시안 혼합 모델을 사용해 클러스터의 개수를 자동선택하는 것"
      ]
    }
  ]
}